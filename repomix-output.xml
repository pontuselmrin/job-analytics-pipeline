This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
scrapers/
  base.py
  scrape_acer.py
  scrape_berec.py
  scrape_ceb.py
  scrape_cedefop.py
  scrape_cor.py
  scrape_easa.py
  scrape_eba.py
  scrape_ebrd.py
  scrape_ecb.py
  scrape_ecdc.py
  scrape_ecmwf.py
  scrape_eda.py
  scrape_edps.py
  scrape_eea.py
  scrape_efsa.py
  scrape_eiopa.py
  scrape_ema.py
  scrape_embl.py
  scrape_emsa.py
  scrape_enisa.py
  scrape_ep.py
  scrape_era.py
  scrape_esa.py
  scrape_esma.py
  scrape_eso.py
  scrape_eu_careers.py
  scrape_euaa.py
  scrape_europol.py
  scrape_eutelsat.py
  scrape_fao.py
  scrape_fra.py
  scrape_frontex.py
  scrape_globalfund.py
  scrape_iaea.py
  scrape_iea.py
  scrape_oecd.py
  scrape_srb.py
  scrape_unesco.py
  scrape_unhcr.py
  scrape_vacancies.py
  scrape_who.py
  scrape_wipo.py
  scrape_wmo.py
  scrape_wto.py
.gitignore
fetch_json.py
log_utils.py
PLAN_NEW_SCRAPERS.md
PROGRESS.md
scrape_eu_temp_agents.py
scrape_log.json
test_scrapers.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "WebFetch(domain:www.acer.europa.eu)",
      "Bash(source:*)",
      "Bash(python3:*)",
      "Bash(python:*)",
      "WebFetch(domain:cpvo.europa.eu)",
      "Bash(curl:*)",
      "WebFetch(domain:e-recruitment.emcdda.europa.eu)",
      "WebFetch(domain:e-recruitment.euda.europa.eu)",
      "WebFetch(domain:careers.cern)",
      "WebFetch(domain:www.ebu.ch)"
    ]
  }
}
</file>

<file path="scrapers/base.py">
"""Shared utilities for all scrapers."""
import re
import time
import requests

DEFAULT_HEADERS = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
}

WORKDAY_HEADERS = {
    "Accept": "application/json",
    "Content-Type": "application/json",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
}

TALEO_HEADERS = {
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "Accept-Language": "en-US,en;q=0.9",
    "Content-Type": "application/json",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
    "X-Requested-With": "XMLHttpRequest",
    "tz": "GMT+01:00",
    "tzname": "Europe/Berlin",
}


def fetch(url, method="GET", headers=None, **kwargs):
    """HTTP request with retry (3 attempts, exponential backoff)."""
    if headers is None:
        headers = DEFAULT_HEADERS
    for attempt in range(3):
        try:
            resp = requests.request(method, url, headers=headers, **kwargs)
            resp.raise_for_status()
            return resp
        except requests.RequestException:
            if attempt == 2:
                raise
            time.sleep(2 ** attempt)


def normalize_url(href, base_url):
    """Return absolute URL: if href starts with http, return as-is, else prepend base_url."""
    if href.startswith("http"):
        return href
    return base_url + href


def extract_links(soup, href_pattern, base_url, min_title_len=5, exclude_patterns=None):
    """Extract deduplicated links from soup where href contains href_pattern.

    Returns list of {"title": ..., "url": ...}.
    """
    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and href_pattern in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        if exclude_patterns and any(p in href for p in exclude_patterns):
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < min_title_len:
            continue

        seen_urls.add(href)
        jobs.append({
            "title": title,
            "url": normalize_url(href, base_url),
        })

    return jobs


def scrape_workday(base_url, api_url):
    """Full Workday pagination loop. Returns list of {"title", "url", "location"}."""
    jobs = []
    offset = 0
    limit = 20

    while True:
        payload = {
            "appliedFacets": {},
            "limit": limit,
            "offset": offset,
            "searchText": "",
        }
        resp = fetch(api_url, method="POST", headers=WORKDAY_HEADERS, json=payload)
        data = resp.json()

        postings = data.get("jobPostings", [])
        if not postings:
            break

        for job in postings:
            jobs.append({
                "title": job.get("title", ""),
                "url": base_url + job.get("externalPath", ""),
                "location": job.get("locationsText", ""),
            })

        offset += limit
        if offset >= data.get("total", 0):
            break

    return jobs


def scrape_taleo(base_url, portal_id, section, column_map, strip_columns=None,
                 filters=None, deduplicate=False):
    """Full Taleo pagination loop.

    column_map: dict mapping column index to field name, e.g. {0: "title", 1: "location"}
    strip_columns: set of column indices whose values should have []" stripped (locations)
    filters: optional list of filter dicts for filterSelectionParam/advancedSearchFiltersSelectionParam
    deduplicate: if True, skip duplicate contestNo values
    """
    api_url = f"{base_url}/careersection/rest/jobboard/searchjobs"
    headers = {
        **TALEO_HEADERS,
        "Origin": base_url,
        "Referer": f"{base_url}/careersection/{section}/jobsearch.ftl?lang=en",
    }
    cookies = {"locale": "en"}
    if strip_columns is None:
        strip_columns = set()

    filter_selections = filters or []

    all_jobs = []
    seen_ids = set()
    page = 1

    while True:
        print(f"Fetching page {page}...")
        payload = {
            "multilineEnabled": True,
            "sortingSelection": {
                "sortBySelectionParam": "3",
                "ascendingSortingOrder": "false",
            },
            "fieldData": {
                "fields": {"KEYWORD": "", "LOCATION": ""},
                "valid": True,
            },
            "filterSelectionParam": {"searchFilterSelections": filter_selections},
            "advancedSearchFiltersSelectionParam": {"searchFilterSelections": filter_selections},
            "pageNo": page,
        }

        resp = fetch(
            api_url,
            method="POST",
            headers=headers,
            json=payload,
            cookies=cookies,
            params={"lang": "en", "portal": portal_id},
        )
        data = resp.json()

        jobs = data.get("requisitionList", [])
        if not jobs:
            break

        new_jobs_count = 0
        for job in jobs:
            job_id = job.get("contestNo", "")

            if deduplicate:
                if job_id in seen_ids:
                    continue
                seen_ids.add(job_id)

            new_jobs_count += 1
            columns = job.get("column", [])
            entry = {}
            for idx, field in column_map.items():
                val = columns[idx] if len(columns) > idx else ""
                if idx in strip_columns:
                    val = val.strip('[]"')
                entry[field] = val

            # Always use contestNo for job_number if not in column_map
            if "job_number" not in entry:
                entry["job_number"] = job_id

            entry["url"] = f"{base_url}/careersection/{section}/jobdetail.ftl?job={job_id}"
            all_jobs.append(entry)

        if deduplicate and new_jobs_count == 0:
            break

        paging = data.get("pagingData", {})
        total_count = paging.get("totalCount", 0)
        if len(all_jobs) >= total_count:
            break

        page += 1
        time.sleep(0.5)

    return all_jobs
</file>

<file path=".gitignore">
*.csv
*.xlsx
</file>

<file path="PLAN_NEW_SCRAPERS.md">
# Plan: Implement Scrapers for Remaining Sites

## Background

The project scrapes job listings from ~100 European/international organization websites. There are currently 44 working scrapers in `scrapers/` that all use shared utilities from `scrapers/base.py`. The file `sites.csv` tracks all sites with columns: `name`, `url`, `scraper`. Sites with a blank `scraper` column still need scrapers implemented.

## Project Structure

- `scrapers/base.py` — Shared utilities: `fetch()`, `normalize_url()`, `extract_links()`, `scrape_workday()`, `scrape_taleo()`
- `scrapers/scrape_*.py` — Individual scrapers, each exporting a `scrape()` function returning `list[dict]` (minimum keys: `title`, `url`)
- `test_scrapers.py` — Test runner that loads all scrapers via `importlib` and calls `scrape()`. Has a `SCRAPER_INFO` dict mapping filename to `(name, url)`.
- `sites.csv` — Master list of all sites with scraper coverage status
- `log_utils.py` — Logging utilities used by test runner

## Scraper Contract

Every scraper must:
1. Live in `scrapers/scrape_<abbrev>.py`
2. Import from `base` (e.g., `from base import fetch, normalize_url, extract_links`)
3. Export a `scrape()` function returning `list[dict]` with at minimum `title` and `url` keys
4. Include `if __name__ == "__main__"` block for standalone testing
5. Be added to `SCRAPER_INFO` in `test_scrapers.py`
6. Have its filename noted in the `scraper` column of `sites.csv`

## Existing Patterns to Reuse

### Pattern A: Simple link extraction
```python
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://example.org"
URL = f"{BASE_URL}/careers"

def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)
```

### Pattern B: Table parsing
```python
from bs4 import BeautifulSoup
from base import fetch, normalize_url

def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.find("table")
    jobs = []
    for row in table.find("tbody").find_all("tr"):
        cells = row.find_all("td")
        link = cells[0].find("a")
        jobs.append({"title": link.get_text(strip=True), "url": normalize_url(link["href"], BASE_URL)})
    return jobs
```

### Pattern C: Paginated HTML (like scrape_eu_careers.py)
```python
def scrape():
    all_jobs = []
    page = 0
    while True:
        jobs = scrape_page(page)
        if not jobs: break
        all_jobs.extend(jobs)
        page += 1
        time.sleep(0.5)
    return all_jobs
```

### Pattern D: SmartRecruiters API (like scrape_oecd.py)
```python
API_URL = "https://api.smartrecruiters.com/v1/companies/COMPANY/postings"
def scrape():
    # offset-based pagination with limit=100
```

### Pattern E: Workday API (4 existing scrapers)
```python
from base import scrape_workday
def scrape():
    return scrape_workday(BASE_URL, API_URL)
```

## Sites to Implement (~26 new scrapers)

### Group 1: Simple HTML scraping (Drupal/WordPress/static)

#### 1. CPVO — `scrape_cpvo.py`
- **URL**: https://cpvo.europa.eu/en/about-us/recruitment#page-search---index
- **Platform**: Drupal
- **Structure**: HTML table with columns (Reserve List, Title, Grade, Initial Date, Valid until). Also has an embedded gestmax iframe.
- **Approach**: Parse the HTML table. If table is empty, try extracting links from the gestmax iframe URL.

#### 2. EU-OSHA — `scrape_eu_osha.py`
- **URL**: https://osha.europa.eu/en/careers
- **Platform**: Drupal
- **Structure**: Accordion/tabbed interface with three views (Open, Evaluation underway, Completed). Uses quicktabs module.
- **Approach**: Parse links from the "Open" tab section. May need to fetch iframe URLs separately.

#### 3. EEAS — `scrape_eeas.py`
- **URL**: https://www.eeas.europa.eu/eeas/vacancies_en?f%5B0%5D=vacancy_site%3AEEAS
- **Platform**: Drupal
- **Structure**: Paginated list, 25 items/page, ~88 pages. Each listing has title (link), location, category, deadline.
- **Approach**: Paginated scraping. Parse article/listing elements. Append `?page=N` for pagination.
- **Note**: CEU (Council of the EU) URL points to same EEAS page — one scraper covers both.

#### 4. EUROFOUND — `scrape_eurofound.py`
- **URL**: https://www.eurofound.europa.eu/en/vacancies
- **Platform**: Next.js with Storyblok CMS
- **Structure**: H3 headings for job titles with plain text dates
- **Approach**: Parse H3 elements and associated links/text

#### 5. EUSPA — `scrape_euspa.py`
- **URL**: https://www.euspa.europa.eu/opportunities/careers
- **Platform**: Drupal
- **Structure**: Card-based with title, reference number, employment type, deadline, external "Apply" link
- **Approach**: Parse card elements

#### 6. EIGE — `scrape_eige.py`
- **URL**: https://eige.europa.eu/about/recruitment
- **Platform**: Drupal
- **Structure**: Views-based card layout with metadata (Type, Reference, Published, Closing date). Has pagination.
- **Approach**: Parse `.views-row` / `.views-field` classes

#### 7. EIT — `scrape_eit.py`
- **URL**: https://www.eit.europa.eu/work-with-us/careers/vacancies/open
- **Platform**: Drupal
- **Structure**: Tab navigation (Open, Ongoing, Closed). Parse the "Open" tab.
- **Approach**: Standard Drupal link/card parsing

#### 8. ETF — `scrape_etf.py`
- **URL**: https://www.etf.europa.eu/en/about/recruitment
- **Platform**: Drupal
- **Structure**: Landing page with card-based links to separate vacancy pages
- **Approach**: Extract links to vacancy pages

#### 9. CEPOL — `scrape_cepol.py`
- **URL**: https://www.cepol.europa.eu/work-us/careers/vacancies
- **Platform**: Drupal/Next.js
- **Structure**: Paragraph-based content with tabs and downloads. May have embedded JSON props.
- **Approach**: Parse rendered HTML or extract from embedded JSON

#### 10. EFTA — `scrape_efta.py`
- **URL**: https://www.efta.int/careers/open-vacancies
- **Platform**: Drupal
- **Structure**: H3 headings followed by paragraph links to external jobs.efta.int portal
- **Approach**: Parse H3 + link elements

#### 11. ICES — `scrape_ices.py`
- **URL**: https://www.ices.dk/about-ICES/Jobs-in-ICES/Pages/default.aspx
- **Platform**: SharePoint 2013/2015
- **Structure**: H5 tags with links, followed by deadline text. Content in `#contentBox`.
- **Approach**: Parse `h5 a` elements

#### 12. CERN — `scrape_cern.py`
- **URL**: https://careers.cern/ (main careers page, not just graduates)
- **Platform**: WordPress with Gutenberg blocks
- **Structure**: Job listings in `.wp-block-post-template` with H3 elements
- **Approach**: Parse WordPress block template elements

#### 13. WCC — `scrape_wcc.py`
- **URL**: https://wcccoe.hire.trakstar.com/?#content
- **Platform**: Trakstar Hire
- **Structure**: Simple HTML — `<a>` tags with `<h3>` title and `<p>` details (location, department, type)
- **Approach**: Parse anchor tags containing h3 elements. Very straightforward.

#### 14. F4E — `scrape_f4e.py`
- **URL**: https://fusionforenergy.europa.eu/vacancies/
- **Platform**: WordPress with GestMax ATS
- **Structure**: Empty divs populated via WordPress REST API at `/wp-json/myplugin/v1/data/`
- **Approach**: Try fetching the REST API endpoint directly. Fallback to parsing static HTML if API works.

#### 15. EUISS — `scrape_euiss.py`
- **URL**: https://www.iss.europa.eu/opportunities
- **Platform**: Drupal with AJAX Views
- **Structure**: Initial 10 items in HTML via `.view-opportunities`, `.views-row`. Pagination via AJAX.
- **Approach**: Parse first page. If pagination needed, make AJAX calls.

#### 16. BIS — `scrape_bis.py`
- **URL**: https://www.bis.org/careers/vacancies.htm
- **Platform**: Custom portal
- **Structure**: May be informational page with links to actual job portal. Need to check.
- **Approach**: Fetch page, extract whatever job links are available.

#### 17. EUMETSAT — `scrape_eumetsat.py`
- **URL**: https://eumetsat.onlyfy.jobs/ (redirects from eumetsat.int/work-us/vacancies)
- **Platform**: Onlyfy jobs platform
- **Approach**: Fetch the Onlyfy page and parse job listings. Check if there's an API.

#### 18. NDF — `scrape_ndf.py`
- **URL**: https://www.ndf.int/contact-us/jobs.html
- **Platform**: Sivuviidakko CMS
- **Structure**: Landing page with links to job listings
- **Approach**: Parse links on the page

#### 19. UNFCCC — `scrape_unfccc.py`
- **URL**: https://unfccc.int/secretariat/employment/recruitment
- **Approach**: Fetch and parse. May have certificate issues — try with `verify=False` if needed.

#### 20. SATCEN — `scrape_satcen.py`
- **URL**: https://www.satcen.europa.eu/recruitment/jobs
- **Approach**: Fetch and parse HTML structure

### Group 2: API-based or adapted from existing scrapers

#### 21. EC — `scrape_ec.py`
- **URL**: https://eu-careers.europa.eu/en/job-opportunities/open-vacancies/ec_vacancies
- **Platform**: Same Drupal EU Careers as `scrape_eu_careers.py`
- **Structure**: HTML table with sortable columns (title, domain, DG, grade, location, dates)
- **Approach**: Copy pattern from `scrape_eu_careers.py` with different URL path

#### 22. Euratom — `scrape_euratom.py`
- **URL**: https://eu-careers.europa.eu/en/job-opportunities/open-for-application
- **Platform**: Same Drupal EU Careers
- **Approach**: Same paginated table approach as `scrape_eu_careers.py`

#### 23. EFSF — `scrape_efsf.py`
- **URL**: https://www.esm.europa.eu/careers/vacancies
- **Platform**: Drupal CMS (ESM website)
- **Structure**: Basic job cards with title, employment type, deadline. Links to Oracle CX portal.
- **Approach**: Scrape the listing page for job titles and external links

#### 24. NATO — `scrape_nato.py`
- **URL**: https://www.nato.int/en/work-with-us/careers/vacancies
- **Platform**: Adobe Experience Manager
- **Structure**: Dynamic content loaded via AJAX endpoint `/_jcr_content/root/container/vacancies.nocache.html`
- **Approach**: Fetch the AJAX partial HTML endpoint directly, parse the HTML fragment

#### 25. Council of Europe — `scrape_coe.py`
- **URL**: https://talents.coe.int/en_GB/careersmarketplace
- **Platform**: Custom recruitment portal
- **API**: Endpoints at `/WidgetOpenPositions` and `/SearchJobs`
- **Approach**: Try the API endpoints directly with POST requests. Parse JSON response.

#### 26. EU-LISA — `scrape_eulisa.py`
- **URL**: https://erecruitment.eulisa.europa.eu/en
- **Platform**: Next.js with Material-UI
- **Structure**: Job data may be in `__NEXT_DATA__` script tag
- **Approach**: Fetch page, extract JSON from `__NEXT_DATA__`, parse job listings from props

## Sites NOT Feasible Without Browser Automation (skip for now)

| Site | Reason |
|------|--------|
| European Council [Council] | Cloudflare blocks requests |
| ECA (Court of Auditors) | SharePoint + gestmax iframe, JS-only |
| EFCA (Fisheries Control) | Cloudflare blocks requests |
| European Ombudsman | Full Angular SPA, no SSR |
| ECHA (Chemicals Agency) | PeopleSoft, requires cookie/auth flow |
| EIB (Investment Bank) | PeopleSoft, requires cookie/auth flow |
| EUIPO (IP Office) | SuccessFactors with heavy SAPUI5 JS framework |
| ESM (Stability Mechanism) | Oracle CX SPA, hash-routed, XHR-only data |
| EUROJUST | TALENToft SPA with AJAX loading |
| EUROCONTROL | SSL certificate verification error |
| IMO (Maritime Org) | Minimal HTML, JS-loaded content |
| EMCDDA (Drugs Agency) | Rebranded to EUDA, URLs redirect to 404 |
| NIB (Nordic Investment Bank) | React hydration with base64-encoded data |
| IOOC (Olive Oil Council) | Dynamic loading, no initial HTML content |
| UNWTO (Tourism Org) | Connection refused |
| EBU (Broadcasting Union) | 403 Forbidden |
| ICO (Coffee Org) | SSL certificate error |
| EUROFIMA | No URL listed in CSV |

## After Implementation

1. Add each new scraper to `SCRAPER_INFO` dict in `test_scrapers.py`
2. Update `sites.csv` scraper column for each new scraper
3. Run `python test_scrapers.py` to verify all scrapers work
4. Some sites may have 0 current vacancies — that's OK if scraper runs without errors
</file>

<file path="scrapers/scrape_acer.py">
"""Scraper for Agency for the Cooperation of Energy Regulators [ACER]"""
from bs4 import BeautifulSoup
from base import fetch

URL = "https://www.acer.europa.eu/the-agency/careers/vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "oraclecloud" in h):
        href = link.get("href")
        if href in seen_urls:
            continue

        prev_heading = link.find_previous(["h2", "h3", "h4"])
        if prev_heading:
            title = prev_heading.get_text(strip=True)
            if title and len(title) > 5:
                jobs.append({"title": title, "url": href})
                seen_urls.add(href)

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_berec.py">
"""Scraper for Body of European Regulators for Electronic Communications [BEREC]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

URL = "https://www.berec.europa.eu/en/vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for table in soup.find_all("table"):
        for row in table.find_all("tr"):
            cells = row.find_all("td")
            if len(cells) >= 4:
                link = cells[0].find("a")
                if link:
                    status = cells[3].get_text(strip=True).lower()
                    if "ongoing" in status:
                        jobs.append({
                            "title": link.get_text(strip=True),
                            "url": normalize_url(link.get("href", ""), "https://www.berec.europa.eu"),
                            "grade": cells[1].get_text(strip=True),
                            "deadline": cells[2].get_text(strip=True),
                            "status": cells[3].get_text(strip=True),
                        })

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} ongoing vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Grade: {job['grade']}, Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_ceb.py">
"""Scraper for Council of Europe Development Bank [CEB]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://jobs.coebank.org"
URL = f"{BASE_URL}/search"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_cedefop.py">
"""Scraper for European Centre for the Development of Vocational Training [CEDEFOP]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

URL = "https://www.cedefop.europa.eu/en/about-cedefop/job-opportunities/vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    table = soup.find("table")
    if not table:
        return jobs

    for row in table.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) >= 5:
            link = cells[0].find("a")
            if link:
                jobs.append({
                    "title": link.get_text(strip=True),
                    "url": normalize_url(link.get("href", ""), "https://www.cedefop.europa.eu"),
                    "reference": cells[1].get_text(strip=True),
                    "type": cells[2].get_text(strip=True),
                    "status": cells[3].get_text(strip=True),
                    "deadline": cells[4].get_text(strip=True),
                })

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Reference: {job['reference']}, Type: {job['type']}")
        print(f"  Status: {job['status']}, Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_cor.py">
"""Scraper for Committee of Regions [CR]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

URL = "https://cor.europa.eu/en/about/work-us/jobs"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for heading in soup.find_all(["h3", "h4"]):
        link = heading.find("a")
        if link and link.get("href") and "/jobs/" in link.get("href", ""):
            jobs.append({
                "title": link.get_text(strip=True),
                "url": normalize_url(link.get("href"), "https://cor.europa.eu"),
            })

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_easa.py">
"""Scraper for European Aviation Safety Agency [EASA]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://careers.easa.europa.eu"
URL = f"{BASE_URL}/search/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_eba.py">
"""Scraper for European Banking Authority [EBA]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://www.careers.eba.europa.eu"
URL = f"{BASE_URL}/en"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "/our-vacancies/" in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 5 or title.lower() == "consult vacancy":
            continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": normalize_url(href, BASE_URL)})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_ebrd.py">
"""Scraper for European Bank for Reconstruction and Development [EBRD]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://jobs.ebrd.com"
URL = f"{BASE_URL}/search/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_ecb.py">
"""Scraper for European Central Bank [ECB]"""
from bs4 import BeautifulSoup
from base import fetch

URL = "https://talent.ecb.europa.eu/careers/SearchJobs"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "JobDetail" in h and "talent.ecb.europa.eu" in h):
        href = link.get("href", "")
        if href in seen_urls or any(x in href for x in ["facebook", "twitter", "linkedin", "mailto"]):
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 3:
            continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": href})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_ecdc.py">
"""Scraper for European Centre for Disease Prevention and Control [ECDC]"""
from bs4 import BeautifulSoup
from base import fetch

BASE_URL = "https://erecruitment.ecdc.europa.eu"
URL = f"{BASE_URL}/?page=advertisement"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "advertisement_display" in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 5:
            continue

        full_url = href if href.startswith("http") else BASE_URL + "/" + href
        seen_urls.add(href)
        jobs.append({"title": title, "url": full_url})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_ecmwf.py">
"""Scraper for European Centre for Medium-Range Weather Forecasts [ECMWF]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://jobs.ecmwf.int"
API_URL = f"{BASE_URL}/Home/_JobCard"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "X-Requested-With": "XMLHttpRequest",
    "Accept": "*/*",
}


def scrape():
    resp = fetch(API_URL, headers=HEADERS)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_ids = set()

    for link in soup.find_all("a", href=lambda h: h and "/Job/JobDetail" in h):
        href = link.get("href", "")

        if "JobId=" in href:
            job_id = href.split("JobId=")[-1]
            if job_id in seen_ids:
                continue
            seen_ids.add(job_id)

        title = link.get_text(strip=True)
        if not title or len(title) < 3:
            continue

        jobs.append({"title": title, "url": normalize_url(href, BASE_URL)})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_eda.py">
"""Scraper for European Defence Agency [EDA]"""
from bs4 import BeautifulSoup
from base import fetch

URL = "https://eda.europa.eu/careers/current-vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for item in soup.find_all("div", class_=lambda x: x and "vacancie" in x.lower()):
        link = item.find("a", href=True)
        if link and "vacancies" in link.get("href", "").lower():
            title = link.get_text(strip=True)
            if title:
                jobs.append({"title": title, "url": link.get("href")})

    if not jobs:
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            if "vacanciesnotice" in href.lower():
                title = link.get_text(strip=True)
                if title and len(title) > 5:
                    jobs.append({"title": title, "url": href})

    seen = set()
    unique_jobs = []
    for job in jobs:
        if job["url"] not in seen:
            seen.add(job["url"])
            unique_jobs.append(job)

    return unique_jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_edps.py">
"""Scraper for European Data Protection Supervisor [EDPS]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://www.edps.europa.eu"
URL = f"{BASE_URL}/about/office-edps/careers/our-vacancies_en"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "vacancy" in h.lower()):
        href = link.get("href", "")
        if href in seen_urls or "administrative-notices" not in href:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 10:
            continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": normalize_url(href, BASE_URL)})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_eea.py">
"""Scraper for European Environment Agency [EEA]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://jobs.eea.europa.eu"
URL = BASE_URL


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for row in soup.find_all("div", class_=lambda c: c and "job_list_row" in c):
        link = row.find("a", href=lambda h: h and "/jobs/" in h)
        if not link:
            continue

        href = link.get("href", "")
        if href in seen_urls or "other-jobs-matching" in href:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 3:
            continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": normalize_url(href, BASE_URL)})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_efsa.py">
"""Scraper for European Food Safety Authority [EFSA]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://careers.efsa.europa.eu"
URL = f"{BASE_URL}/jobs/search"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for row in soup.find_all("div", class_=lambda c: c and "job_list_row" in str(c)):
        link = row.find("a", href=lambda h: h and "/jobs/" in h)
        if not link:
            continue

        href = link.get("href", "")
        if href in seen_urls or "other-jobs-matching" in href:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 3:
            continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": normalize_url(href, BASE_URL)})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_eiopa.py">
"""Scraper for European Insurance and Occupational Pensions Authority [EIOPA]"""
import re
from bs4 import BeautifulSoup
from base import fetch

BASE_URL = "https://eiopa.gestmax.eu"
URL = f"{BASE_URL}/search"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "gestmax.eu" in h and "/search" not in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        match = re.search(r'/\d+/\d+/([^?]+)', href)
        if match:
            title = match.group(1).replace('-', ' ').title()
        else:
            title = link.get_text(strip=True)
            if not title or len(title) < 10:
                continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": href})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_ema.py">
"""Scraper for European Medicines Agency [EMA]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://careers.ema.europa.eu"
URL = f"{BASE_URL}/search/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}\n  {job['url']}\n")
</file>

<file path="scrapers/scrape_embl.py">
"""Scraper for European Molecular Biology Laboratory [EMBL] - Workday API"""
from base import scrape_workday

BASE_URL = "https://embl.wd103.myworkdayjobs.com"
API_URL = f"{BASE_URL}/wday/cxs/embl/EMBL/jobs"


def scrape():
    return scrape_workday(BASE_URL, API_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}\n  {job['url']}\n")
    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_emsa.py">
"""Scraper for European Maritime Safety Agency [EMSA]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://www.emsa.europa.eu"
URL = f"{BASE_URL}/jobs/vacancies.html"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for table in soup.find_all("table"):
        for row in table.find_all("tr"):
            cells = row.find_all("td")
            if len(cells) >= 4:
                link = cells[0].find("a")
                if link:
                    jobs.append({
                        "title": link.get_text(strip=True),
                        "url": normalize_url(link.get("href", ""), BASE_URL),
                        "description": cells[1].get_text(strip=True),
                        "published": cells[2].get_text(strip=True),
                        "deadline": cells[3].get_text(strip=True),
                    })

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Published: {job['published']}, Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_enisa.py">
"""Scraper for European Network and Information Security Agency [ENISA]"""
import time
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://www.enisa.europa.eu"
URL = f"{BASE_URL}/careers"


def scrape_page(page_num):
    url = f"{URL}?page={page_num}" if page_num > 0 else URL
    resp = fetch(url)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    table = soup.find("table")
    if not table:
        return jobs

    for row in table.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) >= 5:
            link = cells[0].find("a")
            if link:
                jobs.append({
                    "title": link.get_text(strip=True),
                    "url": normalize_url(link.get("href", ""), BASE_URL),
                    "reference": cells[1].get_text(strip=True),
                    "type": cells[2].get_text(strip=True),
                    "deadline": cells[3].get_text(strip=True),
                    "status": cells[4].get_text(strip=True),
                })

    return jobs


def scrape():
    all_jobs = []
    page = 0

    while True:
        jobs = scrape_page(page)
        if not jobs:
            break

        all_jobs.extend(jobs)
        page += 1
        time.sleep(0.5)

        if page > 20:
            break

    return all_jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Reference: {job['reference']}, Type: {job['type']}")
        print(f"  Deadline: {job['deadline']}, Status: {job['status']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_ep.py">
"""Scraper for European Parliament [EP] - via gestmax"""
import time
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://apply4ep.gestmax.eu"
URL = f"{BASE_URL}/search/index/lang/en_US"


def scrape_page(page_num):
    url = f"{URL}?page={page_num}" if page_num > 1 else URL
    resp = fetch(url)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for item in soup.find_all("a", class_="list-group-item"):
        href = item.get("href", "")
        if not href:
            continue

        title_elem = item.find("h2", class_="list-group-item-heading")
        title = title_elem.get_text(strip=True) if title_elem else ""

        if title:
            jobs.append({
                "title": title,
                "url": normalize_url(href, BASE_URL),
            })

    return jobs


def scrape():
    all_jobs = []
    page = 1

    while True:
        jobs = scrape_page(page)
        if not jobs:
            break

        all_jobs.extend(jobs)
        page += 1
        time.sleep(0.5)

        if page > 10:
            break

    return all_jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_era.py">
"""Scraper for European Railway Agency [ERA]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://www.era.europa.eu"
URL = f"{BASE_URL}/agency-you/recruitment/vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/vacancy/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_esa.py">
"""Scraper for European Space Agency [ESA]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://jobs.esa.int"
URL = f"{BASE_URL}/search/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_esma.py">
"""Scraper for European Securities and Markets Authority [ESMA]"""
from bs4 import BeautifulSoup
from base import fetch

BASE_URL = "https://esmacareers.adequasys.com"
URL = f"{BASE_URL}/?page=advertisement"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "advertisement_display" in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 5:
            continue

        full_url = href if href.startswith("http") else BASE_URL + "/" + href
        seen_urls.add(href)
        jobs.append({"title": title, "url": full_url})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_eso.py">
"""Scraper for European Southern Observatory [ESO]"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://recruitment.eso.org"
URL = BASE_URL

EXCLUDE = ['conditions', 'grants', 'pes', 'how_to', 'terms', 'eiroforum']


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/jobs/", BASE_URL, exclude_patterns=EXCLUDE)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}\n  {job['url']}\n")
</file>

<file path="scrapers/scrape_eu_careers.py">
"""Scraper for EU Careers"""
import time
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://eu-careers.europa.eu"
LIST_URL = f"{BASE_URL}/en/non-permanent-contract-ec"


def scrape_page(page_num):
    url = f"{LIST_URL}?page={page_num}"
    resp = fetch(url)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    table = soup.find("table")
    if not table:
        return []

    tbody = table.find("tbody")
    if not tbody:
        return []

    for row in tbody.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) < 7:
            continue

        title_link = cells[0].find("a")
        if not title_link:
            continue

        jobs.append({
            "title": title_link.get_text(strip=True),
            "url": normalize_url(title_link.get("href", ""), BASE_URL),
            "domain": cells[1].get_text(strip=True),
            "department": cells[2].get_text(strip=True),
            "grade": cells[3].get_text(strip=True),
            "location": cells[4].get_text(strip=True),
            "publication_date": cells[5].get_text(strip=True),
            "deadline": cells[6].get_text(strip=True),
        })

    return jobs


def scrape():
    all_jobs = []
    page = 0

    while True:
        print(f"Scraping page {page}...")
        jobs = scrape_page(page)

        if not jobs:
            break

        all_jobs.extend(jobs)
        page += 1
        time.sleep(0.5)

    return all_jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"\nTotal jobs found: {len(jobs)}\n")
    for i, job in enumerate(jobs, 1):
        print(f"{i}. {job['title']}")
        print(f"   URL: {job['url']}")
        print(f"   Domain: {job['domain']}")
        print(f"   Department: {job['department']}")
        print(f"   Grade: {job['grade']}")
        print(f"   Location: {job['location']}")
        print(f"   Published: {job['publication_date']}")
        print(f"   Deadline: {job['deadline']}")
        print()
</file>

<file path="scrapers/scrape_euaa.py">
"""Scraper for European Asylum Support Office [EASO/EUAA]"""
from bs4 import BeautifulSoup
from base import fetch

URL = "https://euaa.europa.eu/careers/vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for h4 in soup.find_all("h4"):
        title = h4.get_text(strip=True)
        parent = h4.find_parent()
        if parent:
            link = parent.find("a", href=True)
            if link and "careers.euaa" in link.get("href", ""):
                jobs.append({"title": title, "url": link.get("href")})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_europol.py">
"""Scraper for European Police Office [EUROPOL]"""
import json
import re
from base import fetch

URL = "https://www.europol.europa.eu/careers-procurement/open-vacancies"


def scrape():
    resp = fetch(URL)

    jobs = []
    match = re.search(r'"talentlink"[^[]*(\[.*?\])', resp.text)
    if match:
        try:
            items = json.loads(match.group(1))
            for item in items:
                title = item.get("title", "")
                alias = item.get("alias", "")
                if title and alias:
                    jobs.append({
                        "title": title,
                        "url": "https://www.europol.europa.eu" + alias,
                    })
        except json.JSONDecodeError:
            pass

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_eutelsat.py">
"""Scraper for EUTELSAT"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://careers.eutelsat.com"
URL = f"{BASE_URL}/search/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_fao.py">
"""Scraper for Food and Agricultural Organization [FAO] - Taleo API"""
from base import scrape_taleo

BASE_URL = "https://jobs.fao.org"
PORTAL_ID = "8105120163"
SECTION = "fao_external"

COLUMN_MAP = {
    0: "title",
    1: "job_number",
    2: "category",
    3: "contract_type",
    4: "location",
    5: "posted",
    6: "deadline",
}
STRIP_COLUMNS = {4}


def scrape():
    return scrape_taleo(BASE_URL, PORTAL_ID, SECTION, COLUMN_MAP, strip_columns=STRIP_COLUMNS)


if __name__ == "__main__":
    jobs = scrape()
    print(f"\nFound {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}")
        print(f"  Location: {job['location']}")
        print(f"  Contract: {job['contract_type']}")
        print(f"  Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")

    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_fra.py">
"""Scraper for European Union Agency for Fundamental Rights [FRA]"""
import re
from bs4 import BeautifulSoup
from base import fetch

BASE_URL = "https://fra.gestmax.eu"
URL = f"{BASE_URL}/search"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "gestmax.eu" in h and "/search" not in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        match = re.search(r'/\d+/\d+/([^?]+)', href)
        if match:
            title = match.group(1).replace('-', ' ').title()
        else:
            title = link.get_text(strip=True)
            if not title or len(title) < 10:
                continue

        seen_urls.add(href)
        jobs.append({"title": title, "url": href})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_frontex.py">
"""Scraper for European Agency for the Management of Operational Cooperation at the External Borders [FRONTEX]"""
from bs4 import BeautifulSoup
from base import fetch

URL = "https://www.frontex.europa.eu/careers/vacancies/open-vacancies/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "microsite.frontex" in h):
        href = link.get("href", "")
        if href in seen_urls:
            continue

        parent = link.find_parent(["div", "article", "section", "li"])
        for _ in range(5):
            if parent:
                heading = parent.find(["h1", "h2", "h3", "h4", "h5", "strong", "b"])
                if heading:
                    title = heading.get_text(strip=True)
                    if title and len(title) > 10 and "view" not in title.lower():
                        jobs.append({"title": title, "url": href})
                        seen_urls.add(href)
                        break
                parent = parent.find_parent(["div", "article", "section", "li"])

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_globalfund.py">
"""Scraper for The Global Fund - Workday API"""
from base import scrape_workday

BASE_URL = "https://theglobalfund.wd1.myworkdayjobs.com"
API_URL = f"{BASE_URL}/wday/cxs/theglobalfund/External/jobs"


def scrape():
    return scrape_workday(BASE_URL, API_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}\n  {job['url']}\n")
</file>

<file path="scrapers/scrape_iaea.py">
"""Scraper for International Atomic Energy Agency [IAEA] - Taleo API"""
from base import scrape_taleo

BASE_URL = "https://iaea.taleo.net"
PORTAL_ID = "8105100373"
SECTION = "ex"

COLUMN_MAP = {
    0: "title",
    1: "location",
    2: "grade",
    3: "contract_type",
    4: "deadline",
}
STRIP_COLUMNS = {1}


def scrape():
    return scrape_taleo(
        BASE_URL, PORTAL_ID, SECTION, COLUMN_MAP,
        strip_columns=STRIP_COLUMNS, deduplicate=True,
    )


if __name__ == "__main__":
    jobs = scrape()
    print(f"\nFound {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Location: {job['location']}")
        print(f"  Grade: {job['grade']}")
        print(f"  Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_iea.py">
"""Scraper for International Energy Agency [IEA] - SmartRecruiters HTML"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

URL = "https://careers.smartrecruiters.com/OECD/iea"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "jobs.smartrecruiters.com", "", min_title_len=5)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_oecd.py">
"""Scraper for OECD - uses SmartRecruiters API"""
from base import fetch, DEFAULT_HEADERS

API_URL = "https://api.smartrecruiters.com/v1/companies/OECD/postings"


def scrape():
    jobs = []
    offset = 0
    limit = 100

    while True:
        resp = fetch(API_URL, headers=DEFAULT_HEADERS, params={"limit": limit, "offset": offset})
        data = resp.json()

        postings = data.get("content", [])
        if not postings:
            break

        for posting in postings:
            location = posting.get("location", {})
            jobs.append({
                "title": posting.get("name", ""),
                "url": f"https://jobs.smartrecruiters.com/OECD/{posting.get('id', '')}",
                "department": posting.get("department", {}).get("label", ""),
                "location": f"{location.get('city', '')}, {location.get('country', '').upper()}",
                "posted": posting.get("releasedDate", ""),
            })

        if len(postings) < limit:
            break
        offset += limit

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Department: {job['department']}")
        print(f"  Location: {job['location']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_srb.py">
"""Scraper for Single Resolution Board [SRB]"""
from bs4 import BeautifulSoup
from base import fetch, normalize_url

BASE_URL = "https://www.srb.europa.eu"
URL = f"{BASE_URL}/en/vacancies"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    for h3 in soup.find_all("h3"):
        link = h3.find("a")
        if link and link.get("href"):
            title = link.get_text(strip=True)
            if title:
                jobs.append({
                    "title": title,
                    "url": normalize_url(link.get("href", ""), BASE_URL),
                })

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_unesco.py">
"""Scraper for UNESCO"""
from bs4 import BeautifulSoup
from base import fetch, extract_links

BASE_URL = "https://careers.unesco.org"
URL = f"{BASE_URL}/go/All-jobs-openings/784002/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")
    return extract_links(soup, "/job/", BASE_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_unhcr.py">
"""Scraper for UNHCR - Workday API"""
from base import scrape_workday

BASE_URL = "https://unhcr.wd3.myworkdayjobs.com"
API_URL = f"{BASE_URL}/wday/cxs/unhcr/External/jobs"


def scrape():
    return scrape_workday(BASE_URL, API_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}\n  {job['url']}\n")
    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_vacancies.py">
"""Scraper for AMLA vacancies"""
from bs4 import BeautifulSoup
from base import fetch

URL = "https://www.amla.europa.eu/careers/vacancies_en"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    external_heading = None
    for h2 in soup.find_all("h2"):
        if "Ongoing selection procedures - External" in h2.get_text():
            external_heading = h2
            break

    if not external_heading:
        return []

    table = external_heading.find_next("table")
    if not table:
        return []

    vacancies = []
    tbody = table.find("tbody")
    rows = tbody.find_all("tr") if tbody else table.find_all("tr")[1:]

    for row in rows:
        cells = row.find_all(["td", "th"])
        if len(cells) >= 4:
            vacancy = {
                "title": cells[0].get_text(strip=True),
                "contract_type": cells[1].get_text(strip=True),
                "grade": cells[2].get_text(strip=True),
                "status": cells[3].get_text(strip=True),
            }
            link = cells[0].find("a")
            if link and link.get("href"):
                vacancy["url"] = link.get("href")
            vacancies.append(vacancy)

    return vacancies


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} external vacancies:\n")
    for v in jobs:
        print(f"Title: {v['title']}")
        print(f"  Contract: {v['contract_type']}")
        print(f"  Grade: {v['grade']}")
        print(f"  Status: {v['status']}")
        if "url" in v:
            print(f"  URL: {v['url']}")
        print()
</file>

<file path="scrapers/scrape_who.py">
"""Scraper for World Health Organization [WHO] - Taleo API"""
from base import scrape_taleo

BASE_URL = "https://careers.who.int"
PORTAL_ID = "101430233"
SECTION = "ex"

COLUMN_MAP = {
    0: "title",
    1: "job_number",
    2: "location",
    3: "grade",
    4: "contract_type",
    5: "deadline",
    6: "organization",
}
STRIP_COLUMNS = {2}

FILTERS = [
    {"id": "POSTING_DATE", "selectedValues": []},
    {"id": "LOCATION", "selectedValues": []},
    {"id": "JOB_FIELD", "selectedValues": []},
    {"id": "JOB_TYPE", "selectedValues": []},
    {"id": "JOB_SCHEDULE", "selectedValues": []},
    {"id": "JOB_LEVEL", "selectedValues": []},
    {"id": "EMPLOYEE_STATUS", "selectedValues": []},
]


def scrape():
    return scrape_taleo(
        BASE_URL, PORTAL_ID, SECTION, COLUMN_MAP,
        strip_columns=STRIP_COLUMNS, filters=FILTERS,
    )


if __name__ == "__main__":
    jobs = scrape()
    print(f"\nFound {len(jobs)} vacancies:\n")
    for job in jobs[:10]:
        print(f"- {job['title']}")
        print(f"  Location: {job['location']}")
        print(f"  Grade: {job['grade']}")
        print(f"  Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")

    if len(jobs) > 10:
        print(f"... and {len(jobs) - 10} more")
</file>

<file path="scrapers/scrape_wipo.py">
"""Scraper for World Intellectual Property Organization [WIPO] - Taleo API"""
from base import scrape_taleo

BASE_URL = "https://wipo.taleo.net"
PORTAL_ID = "42105027338"
SECTION = "wp_2_pd"

COLUMN_MAP = {
    0: "title",
    1: "grade",
    2: "department",
    3: "job_number",
    4: "contract_type",
    6: "location",
    8: "deadline",
}
STRIP_COLUMNS = {6}


def scrape():
    return scrape_taleo(BASE_URL, PORTAL_ID, SECTION, COLUMN_MAP, strip_columns=STRIP_COLUMNS)


if __name__ == "__main__":
    jobs = scrape()
    print(f"\nFound {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  Location: {job['location']}")
        print(f"  Grade: {job['grade']}")
        print(f"  Deadline: {job['deadline']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_wmo.py">
"""Scraper for World Meteorological Organization [WMO]"""
import re
from bs4 import BeautifulSoup
from base import fetch

BASE_URL = "https://erecruit.wmo.int/public"
URL = f"{BASE_URL}/"


def scrape():
    resp = fetch(URL)
    soup = BeautifulSoup(resp.text, "html.parser")

    jobs = []
    seen_urls = set()

    for link in soup.find_all("a", href=lambda h: h and "hrd-cl-vac-view.asp" in h):
        href = link.get("href", "")

        match = re.search(r'jobinfo_uid_c=(\d+)', href)
        if not match:
            continue
        job_id = match.group(1)
        if job_id in seen_urls:
            continue

        title = link.get_text(strip=True)
        if not title or len(title) < 5 or re.match(r'^\d+', title):
            continue

        clean_url = f"{BASE_URL}/hrd-cl-vac-view.asp?jobinfo_uid_c={job_id}&vaclng=en"
        seen_urls.add(job_id)
        jobs.append({"title": title, "url": clean_url})

    return jobs


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="scrapers/scrape_wto.py">
"""Scraper for World Trade Organization [WTO] - Workday API"""
from base import scrape_workday

BASE_URL = "https://wto.wd103.myworkdayjobs.com"
API_URL = f"{BASE_URL}/wday/cxs/wto/External/jobs"


def scrape():
    return scrape_workday(BASE_URL, API_URL)


if __name__ == "__main__":
    jobs = scrape()
    print(f"Found {len(jobs)} vacancies:\n")
    for job in jobs:
        print(f"- {job['title']}")
        print(f"  {job['url']}\n")
</file>

<file path="fetch_json.py">
import requests

url = "https://webtools.europa.eu/rest/service-inventory"

headers = {
    "accept": "*/*",
    "accept-language": "en-US,en;q=0.9",
    "content-type": "application/x-www-form-urlencoded",
    "origin": "https://www.amla.europa.eu",
    "referer": "https://www.amla.europa.eu/",
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
}

data = {
    "url": "https://www.amla.europa.eu/careers/vacancies_en",
    "lang": "en",
    "components": '[{"service":"preview","version":null,"provider":null,"id":null,"url":null,"maptype":null},{"service":"etrans","version":null,"provider":null,"id":null,"url":null,"maptype":null}]',
}

response = requests.post(url, headers=headers, data=data)

if response.ok:
    print(response.json())
else:
    print(f"Error: {response.status_code}")
    print(response.text)
</file>

<file path="log_utils.py">
import json
from datetime import datetime
from pathlib import Path

LOG_FILE = Path(__file__).parent / "scrape_log.json"


def load_log():
    with open(LOG_FILE) as f:
        return json.load(f)


def save_log(log):
    log["last_updated"] = datetime.now().isoformat()
    log["summary"]["total"] = len(log["sites"])
    log["summary"]["success"] = sum(1 for s in log["sites"] if s["status"] == "success")
    log["summary"]["partial"] = sum(1 for s in log["sites"] if s["status"] == "partial")
    log["summary"]["failed"] = sum(1 for s in log["sites"] if s["status"] == "failed")
    log["summary"]["skipped"] = sum(1 for s in log["sites"] if s["status"] == "skipped")
    with open(LOG_FILE, "w") as f:
        json.dump(log, f, indent=2)


def log_site(name, url, status, script_file=None, vacancies_found=None,
             has_pagination=False, notes=""):
    """
    Log a site's scraping outcome.

    status: "success" | "partial" | "failed" | "skipped"
    """
    log = load_log()

    # Remove existing entry if present
    log["sites"] = [s for s in log["sites"] if s["name"] != name]

    log["sites"].append({
        "name": name,
        "url": url,
        "status": status,
        "script_file": script_file,
        "vacancies_found": vacancies_found,
        "has_pagination": has_pagination,
        "notes": notes,
        "timestamp": datetime.now().isoformat()
    })

    save_log(log)
    print(f"[{status.upper()}] {name}: {notes}")


def show_progress():
    log = load_log()
    s = log["summary"]
    print(f"\nProgress: {s['total']} sites processed")
    print(f"  Success: {s['success']}")
    print(f"  Partial: {s['partial']}")
    print(f"  Failed:  {s['failed']}")
    print(f"  Skipped: {s['skipped']}")
</file>

<file path="PROGRESS.md">
# Scraping Project Progress

## Summary
Building scrapers for ~88 job vacancy sites from `sites.csv`.


## Files Structure
```
/scrapers/           - 42 scraper scripts (each has scrape() function)
/test_scrapers.py    - Test runner for all scrapers
/scrape_log.json     - Results log (auto-updated by test runner)
/log_utils.py        - Logging utilities (log_site, show_progress)
/sites.csv           - Source list of ~88 sites
/venv/               - Python virtual environment
/PROGRESS.md         - This file
```


## API Patterns Reference

### Taleo
```
POST https://{domain}/careersection/rest/jobboard/searchjobs
Headers:
  tz: GMT+01:00
  tzname: Europe/Berlin
  Content-Type: application/json
Body: {"multilineEnabled":true,"sortingSelection":{"sortBySelectionParam":"3","ascendingSortingOrder":"false"},"fieldData":{"fields":{"KEYWORD":"","LOCATION":""},"valid":true},"filterSelectionParam":{"searchFilterSelections":[{"id":"POSTING_DATE","selectedValues":[]},{"id":"LOCATION","selectedValues":[]},{"id":"JOB_FIELD","selectedValues":[]}]},"pageNo":1}
```

### Workday
```
POST https://{company}.wd{N}.myworkdayjobs.com/wday/cxs/{company}/{site}/jobs
Headers:
  Content-Type: application/json
  Accept: application/json
Body: {"appliedFacets":{},"limit":20,"offset":0,"searchText":""}
```

### SmartRecruiters
```
GET https://api.smartrecruiters.com/v1/companies/{COMPANY}/postings?offset=0&limit=100
```

### ECMWF-style
```
GET /Home/_JobCard?Skip=0
Headers:
  X-Requested-With: XMLHttpRequest
```
</file>

<file path="scrape_eu_temp_agents.py">
import requests
from bs4 import BeautifulSoup
import time

BASE_URL = "https://eu-careers.europa.eu"
LIST_URL = f"{BASE_URL}/en/temporary-agents-other-institutions-vacancies"

headers = {
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36",
}


def scrape_page(page_num):
    """Scrape a single page of job listings."""
    url = f"{LIST_URL}?page={page_num}"
    response = requests.get(url, headers=headers)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")
    jobs = []

    table = soup.find("table")
    if not table:
        return []

    tbody = table.find("tbody")
    if not tbody:
        return []

    rows = tbody.find_all("tr")
    for row in rows:
        cells = row.find_all("td")
        if len(cells) < 7:
            continue

        title_cell = cells[0]
        title_link = title_cell.find("a")
        if not title_link:
            continue

        job = {
            "title": title_link.get_text(strip=True),
            "url": BASE_URL + title_link.get("href") if title_link.get("href") else None,
            "domain": cells[1].get_text(strip=True),
            "grade": cells[2].get_text(strip=True),
            "institution": cells[3].get_text(strip=True),
            "location": cells[4].get_text(strip=True),
            "publication_date": cells[5].get_text(strip=True),
            "deadline": cells[6].get_text(strip=True),
        }
        jobs.append(job)

    return jobs


def scrape_all_jobs():
    """Scrape all pages of job listings."""
    all_jobs = []
    page = 0

    while True:
        print(f"Scraping page {page}...")
        jobs = scrape_page(page)

        if not jobs:
            print(f"No jobs found on page {page}, stopping.")
            break

        all_jobs.extend(jobs)
        print(f"  Found {len(jobs)} jobs (total: {len(all_jobs)})")

        page += 1
        time.sleep(0.5)

    return all_jobs


if __name__ == "__main__":
    jobs = scrape_all_jobs()

    print(f"\n{'='*60}")
    print(f"Total jobs found: {len(jobs)}")
    print(f"{'='*60}\n")

    for i, job in enumerate(jobs, 1):
        print(f"{i}. {job['title']}")
        print(f"   URL: {job['url']}")
        print(f"   Domain: {job['domain']}")
        print(f"   Grade: {job['grade']}")
        print(f"   Institution: {job['institution']}")
        print(f"   Location: {job['location']}")
        print(f"   Published: {job['publication_date']}")
        print(f"   Deadline: {job['deadline']}")
        print()
</file>

<file path="scrape_log.json">
{
  "last_updated": "2026-02-12T16:38:13.183723",
  "summary": {
    "total": 42,
    "success": 41,
    "partial": 0,
    "failed": 1,
    "skipped": 0
  },
  "sites": [
    {
      "name": "Agency for the Cooperation of Energy Regulators [ACER]",
      "url": "https://www.acer.europa.eu/the-agency/careers/vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_acer.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:37:33.699187"
    },
    {
      "name": "Body of European Regulators for Electronic Communications [BEREC]",
      "url": "https://www.berec.europa.eu/en/vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_berec.py",
      "vacancies_found": 2,
      "has_pagination": false,
      "notes": "Found 2 jobs",
      "timestamp": "2026-02-12T16:37:34.091650"
    },
    {
      "name": "Committee of Regions [CR]",
      "url": "https://cor.europa.eu/en/about/work-us/jobs",
      "status": "success",
      "script_file": "scrapers/scrape_cor.py",
      "vacancies_found": 4,
      "has_pagination": false,
      "notes": "Found 4 jobs",
      "timestamp": "2026-02-12T16:37:34.726730"
    },
    {
      "name": "European Agency for the Management of Operational Cooperation at the External Borders [FRONTEX]",
      "url": "https://www.frontex.europa.eu/careers/vacancies/open-vacancies/",
      "status": "success",
      "script_file": "scrapers/scrape_frontex.py",
      "vacancies_found": 2,
      "has_pagination": false,
      "notes": "Found 2 jobs",
      "timestamp": "2026-02-12T16:37:34.900683"
    },
    {
      "name": "European Asylum Support Office [EASO]",
      "url": "https://euaa.europa.eu/careers/vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_euaa.py",
      "vacancies_found": 8,
      "has_pagination": false,
      "notes": "Found 8 jobs",
      "timestamp": "2026-02-12T16:37:35.294603"
    },
    {
      "name": "European Centre for the Development of Vocational Training [CEDEFOP]",
      "url": "https://www.cedefop.europa.eu/en/about-cedefop/job-opportunities/vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_cedefop.py",
      "vacancies_found": 10,
      "has_pagination": false,
      "notes": "Found 10 jobs",
      "timestamp": "2026-02-12T16:37:35.544732"
    },
    {
      "name": "European Defence Agency [EDA]",
      "url": "https://eda.europa.eu/careers/current-vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_eda.py",
      "vacancies_found": 7,
      "has_pagination": false,
      "notes": "Found 7 jobs",
      "timestamp": "2026-02-12T16:37:36.760518"
    },
    {
      "name": "European Maritime Safety Agency [EMSA]",
      "url": "https://www.emsa.europa.eu/jobs/vacancies.html",
      "status": "success",
      "script_file": "scrapers/scrape_emsa.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:37:37.456537"
    },
    {
      "name": "European Network and Information Security Agency [ENISA]",
      "url": "https://www.enisa.europa.eu/careers",
      "status": "success",
      "script_file": "scrapers/scrape_enisa.py",
      "vacancies_found": 132,
      "has_pagination": false,
      "notes": "Found 132 jobs",
      "timestamp": "2026-02-12T16:37:47.841645"
    },
    {
      "name": "Single Resolution Board [SRB]",
      "url": "https://www.srb.europa.eu/en/vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_srb.py",
      "vacancies_found": 8,
      "has_pagination": false,
      "notes": "Found 8 jobs",
      "timestamp": "2026-02-12T16:37:48.016589"
    },
    {
      "name": "European Police Office [EUROPOL]",
      "url": "https://www.europol.europa.eu/careers-procurement/open-vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_europol.py",
      "vacancies_found": 9,
      "has_pagination": false,
      "notes": "Found 9 jobs",
      "timestamp": "2026-02-12T16:37:48.141005"
    },
    {
      "name": "European Parliament [EP]",
      "url": "https://apply4ep.gestmax.eu/search/index/lang/en_US",
      "status": "success",
      "script_file": "scrapers/scrape_ep.py",
      "vacancies_found": 1,
      "has_pagination": false,
      "notes": "Found 1 jobs",
      "timestamp": "2026-02-12T16:37:49.081271"
    },
    {
      "name": "Organisation for Economic Co-operation and Development [OECD]",
      "url": "https://careers.smartrecruiters.com/OECD",
      "status": "success",
      "script_file": "scrapers/scrape_oecd.py",
      "vacancies_found": 28,
      "has_pagination": false,
      "notes": "Found 28 jobs",
      "timestamp": "2026-02-12T16:37:49.276266"
    },
    {
      "name": "World Health Organization [WHO]",
      "url": "https://careers.who.int/careersection/ex/jobsearch.ftl",
      "status": "success",
      "script_file": "scrapers/scrape_who.py",
      "vacancies_found": 60,
      "has_pagination": false,
      "notes": "Found 60 jobs",
      "timestamp": "2026-02-12T16:37:52.503311"
    },
    {
      "name": "Food and Agricultural Organization [FAO]",
      "url": "https://jobs.fao.org/careersection/fao_external/jobsearch.ftl",
      "status": "success",
      "script_file": "scrapers/scrape_fao.py",
      "vacancies_found": 137,
      "has_pagination": false,
      "notes": "Found 137 jobs",
      "timestamp": "2026-02-12T16:37:58.883233"
    },
    {
      "name": "World Intellectual Property Organization [WIPO]",
      "url": "https://wipo.taleo.net/careersection/wp_2_pd/jobsearch.ftl",
      "status": "success",
      "script_file": "scrapers/scrape_wipo.py",
      "vacancies_found": 11,
      "has_pagination": false,
      "notes": "Found 11 jobs",
      "timestamp": "2026-02-12T16:37:59.425121"
    },
    {
      "name": "International Atomic Energy Agency [IAEA]",
      "url": "https://iaea.taleo.net/careersection/ex/jobsearch.ftl",
      "status": "success",
      "script_file": "scrapers/scrape_iaea.py",
      "vacancies_found": 27,
      "has_pagination": false,
      "notes": "Found 27 jobs",
      "timestamp": "2026-02-12T16:38:01.502838"
    },
    {
      "name": "European Central Bank [ECB]",
      "url": "https://talent.ecb.europa.eu/careers/SearchJobs",
      "status": "success",
      "script_file": "scrapers/scrape_ecb.py",
      "vacancies_found": 10,
      "has_pagination": false,
      "notes": "Found 10 jobs",
      "timestamp": "2026-02-12T16:38:02.276588"
    },
    {
      "name": "European Environment Agency [EEA]",
      "url": "https://jobs.eea.europa.eu/",
      "status": "success",
      "script_file": "scrapers/scrape_eea.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:38:02.540044"
    },
    {
      "name": "European Railway Agency [ERA]",
      "url": "https://www.era.europa.eu/agency-you/recruitment/vacancies",
      "status": "success",
      "script_file": "scrapers/scrape_era.py",
      "vacancies_found": 10,
      "has_pagination": false,
      "notes": "Found 10 jobs",
      "timestamp": "2026-02-12T16:38:02.677486"
    },
    {
      "name": "European Data Protection Supervisor [EDPS]",
      "url": "https://www.edps.europa.eu/about/office-edps/careers/our-vacancies_en",
      "status": "success",
      "script_file": "scrapers/scrape_edps.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:38:02.862349"
    },
    {
      "name": "European Insurance and Occupational Pensions Authority [EIOPA]",
      "url": "https://eiopa.gestmax.eu/search",
      "status": "success",
      "script_file": "scrapers/scrape_eiopa.py",
      "vacancies_found": 5,
      "has_pagination": false,
      "notes": "Found 5 jobs",
      "timestamp": "2026-02-12T16:38:03.091566"
    },
    {
      "name": "European Food Safety Authority [EFSA]",
      "url": "https://careers.efsa.europa.eu/jobs/search",
      "status": "success",
      "script_file": "scrapers/scrape_efsa.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:38:03.402048"
    },
    {
      "name": "European Centre for Disease Prevention and Control [ECDC]",
      "url": "https://erecruitment.ecdc.europa.eu/?page=advertisement",
      "status": "success",
      "script_file": "scrapers/scrape_ecdc.py",
      "vacancies_found": 2,
      "has_pagination": false,
      "notes": "Found 2 jobs",
      "timestamp": "2026-02-12T16:38:03.588380"
    },
    {
      "name": "European Union Agency for Fundamental Rights [FRA]",
      "url": "https://fra.gestmax.eu/search",
      "status": "failed",
      "script_file": null,
      "vacancies_found": null,
      "has_pagination": false,
      "notes": "No jobs found (may be empty or scraper issue)",
      "timestamp": "2026-02-12T16:38:03.815588"
    },
    {
      "name": "European Banking Authority [EBA]",
      "url": "https://www.careers.eba.europa.eu/en",
      "status": "success",
      "script_file": "scrapers/scrape_eba.py",
      "vacancies_found": 2,
      "has_pagination": false,
      "notes": "Found 2 jobs",
      "timestamp": "2026-02-12T16:38:05.369705"
    },
    {
      "name": "European Securities and Markets Authority [ESMA]",
      "url": "https://esmacareers.adequasys.com/?page=advertisement",
      "status": "success",
      "script_file": "scrapers/scrape_esma.py",
      "vacancies_found": 1,
      "has_pagination": false,
      "notes": "Found 1 jobs",
      "timestamp": "2026-02-12T16:38:05.530340"
    },
    {
      "name": "International Energy Agency [IEA]",
      "url": "https://careers.smartrecruiters.com/OECD/iea",
      "status": "success",
      "script_file": "scrapers/scrape_iea.py",
      "vacancies_found": 2,
      "has_pagination": false,
      "notes": "Found 2 jobs",
      "timestamp": "2026-02-12T16:38:06.079861"
    },
    {
      "name": "Council of Europe Development Bank [CEB]",
      "url": "https://jobs.coebank.org/search",
      "status": "success",
      "script_file": "scrapers/scrape_ceb.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:38:06.424772"
    },
    {
      "name": "European Space Agency [ESA]",
      "url": "https://jobs.esa.int/search/",
      "status": "success",
      "script_file": "scrapers/scrape_esa.py",
      "vacancies_found": 25,
      "has_pagination": false,
      "notes": "Found 25 jobs",
      "timestamp": "2026-02-12T16:38:06.844068"
    },
    {
      "name": "UNESCO",
      "url": "https://careers.unesco.org/go/All-jobs-openings/784002/",
      "status": "success",
      "script_file": "scrapers/scrape_unesco.py",
      "vacancies_found": 25,
      "has_pagination": false,
      "notes": "Found 25 jobs",
      "timestamp": "2026-02-12T16:38:07.195468"
    },
    {
      "name": "World Meteorological Organization [WMO]",
      "url": "https://erecruit.wmo.int/public/",
      "status": "success",
      "script_file": "scrapers/scrape_wmo.py",
      "vacancies_found": 15,
      "has_pagination": false,
      "notes": "Found 15 jobs",
      "timestamp": "2026-02-12T16:38:07.532407"
    },
    {
      "name": "European Aviation Safety Agency [EASA]",
      "url": "https://careers.easa.europa.eu/search/",
      "status": "success",
      "script_file": "scrapers/scrape_easa.py",
      "vacancies_found": 4,
      "has_pagination": false,
      "notes": "Found 4 jobs",
      "timestamp": "2026-02-12T16:38:07.805713"
    },
    {
      "name": "EUTELSAT",
      "url": "https://careers.eutelsat.com/search/",
      "status": "success",
      "script_file": "scrapers/scrape_eutelsat.py",
      "vacancies_found": 25,
      "has_pagination": false,
      "notes": "Found 25 jobs",
      "timestamp": "2026-02-12T16:38:08.244444"
    },
    {
      "name": "European Bank for Reconstruction and Development [EBRD]",
      "url": "https://jobs.ebrd.com/search/",
      "status": "success",
      "script_file": "scrapers/scrape_ebrd.py",
      "vacancies_found": 25,
      "has_pagination": false,
      "notes": "Found 25 jobs",
      "timestamp": "2026-02-12T16:38:08.466438"
    },
    {
      "name": "European Southern Observatory [ESO]",
      "url": "https://recruitment.eso.org/",
      "status": "success",
      "script_file": "scrapers/scrape_eso.py",
      "vacancies_found": 13,
      "has_pagination": false,
      "notes": "Found 13 jobs",
      "timestamp": "2026-02-12T16:38:08.916136"
    },
    {
      "name": "European Medicines Agency [EMA]",
      "url": "https://careers.ema.europa.eu/search/",
      "status": "success",
      "script_file": "scrapers/scrape_ema.py",
      "vacancies_found": 1,
      "has_pagination": false,
      "notes": "Found 1 jobs",
      "timestamp": "2026-02-12T16:38:09.139515"
    },
    {
      "name": "European Centre for Medium-Range Weather Forecasts [ECMWF]",
      "url": "https://jobs.ecmwf.int/Home/Job",
      "status": "success",
      "script_file": "scrapers/scrape_ecmwf.py",
      "vacancies_found": 3,
      "has_pagination": false,
      "notes": "Found 3 jobs",
      "timestamp": "2026-02-12T16:38:09.406108"
    },
    {
      "name": "World Trade Organization [WTO]",
      "url": "https://wto.wd103.myworkdayjobs.com/External",
      "status": "success",
      "script_file": "scrapers/scrape_wto.py",
      "vacancies_found": 2,
      "has_pagination": false,
      "notes": "Found 2 jobs",
      "timestamp": "2026-02-12T16:38:09.764608"
    },
    {
      "name": "European Molecular Biology Laboratory [EMBL]",
      "url": "https://embl.wd103.myworkdayjobs.com/EMBL",
      "status": "success",
      "script_file": "scrapers/scrape_embl.py",
      "vacancies_found": 31,
      "has_pagination": false,
      "notes": "Found 31 jobs",
      "timestamp": "2026-02-12T16:38:10.848736"
    },
    {
      "name": "UNHCR",
      "url": "https://unhcr.wd3.myworkdayjobs.com/External",
      "status": "success",
      "script_file": "scrapers/scrape_unhcr.py",
      "vacancies_found": 40,
      "has_pagination": false,
      "notes": "Found 40 jobs",
      "timestamp": "2026-02-12T16:38:12.283475"
    },
    {
      "name": "The Global Fund",
      "url": "https://theglobalfund.wd1.myworkdayjobs.com/External",
      "status": "success",
      "script_file": "scrapers/scrape_globalfund.py",
      "vacancies_found": 1,
      "has_pagination": false,
      "notes": "Found 1 jobs",
      "timestamp": "2026-02-12T16:38:13.183720"
    }
  ]
}
</file>

<file path="test_scrapers.py">
"""Test all scrapers and log results"""
import sys
import importlib.util
from pathlib import Path

# Add parent to path for log_utils, and scrapers dir for base module
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent / "scrapers"))
from log_utils import log_site, show_progress

SCRAPERS_DIR = Path(__file__).parent / "scrapers"

# Map scraper files to site info
SCRAPER_INFO = {
    # EU Agencies - HTML scraping
    "scrape_acer.py": ("Agency for the Cooperation of Energy Regulators [ACER]", "https://www.acer.europa.eu/the-agency/careers/vacancies"),
    "scrape_berec.py": ("Body of European Regulators for Electronic Communications [BEREC]", "https://www.berec.europa.eu/en/vacancies"),
    "scrape_cor.py": ("Committee of Regions [CR]", "https://cor.europa.eu/en/about/work-us/jobs"),
    "scrape_frontex.py": ("European Agency for the Management of Operational Cooperation at the External Borders [FRONTEX]", "https://www.frontex.europa.eu/careers/vacancies/open-vacancies/"),
    "scrape_euaa.py": ("European Asylum Support Office [EASO]", "https://euaa.europa.eu/careers/vacancies"),
    "scrape_cedefop.py": ("European Centre for the Development of Vocational Training [CEDEFOP]", "https://www.cedefop.europa.eu/en/about-cedefop/job-opportunities/vacancies"),
    "scrape_eda.py": ("European Defence Agency [EDA]", "https://eda.europa.eu/careers/current-vacancies"),
    "scrape_emsa.py": ("European Maritime Safety Agency [EMSA]", "https://www.emsa.europa.eu/jobs/vacancies.html"),
    "scrape_enisa.py": ("European Network and Information Security Agency [ENISA]", "https://www.enisa.europa.eu/careers"),
    "scrape_srb.py": ("Single Resolution Board [SRB]", "https://www.srb.europa.eu/en/vacancies"),
    "scrape_europol.py": ("European Police Office [EUROPOL]", "https://www.europol.europa.eu/careers-procurement/open-vacancies"),
    "scrape_ep.py": ("European Parliament [EP]", "https://apply4ep.gestmax.eu/search/index/lang/en_US"),
    # API-based scrapers
    "scrape_oecd.py": ("Organisation for Economic Co-operation and Development [OECD]", "https://careers.smartrecruiters.com/OECD"),
    # Taleo API scrapers
    "scrape_who.py": ("World Health Organization [WHO]", "https://careers.who.int/careersection/ex/jobsearch.ftl"),
    "scrape_fao.py": ("Food and Agricultural Organization [FAO]", "https://jobs.fao.org/careersection/fao_external/jobsearch.ftl"),
    "scrape_wipo.py": ("World Intellectual Property Organization [WIPO]", "https://wipo.taleo.net/careersection/wp_2_pd/jobsearch.ftl"),
    "scrape_iaea.py": ("International Atomic Energy Agency [IAEA]", "https://iaea.taleo.net/careersection/ex/jobsearch.ftl"),
    # More EU/International agencies
    "scrape_ecb.py": ("European Central Bank [ECB]", "https://talent.ecb.europa.eu/careers/SearchJobs"),
    "scrape_eea.py": ("European Environment Agency [EEA]", "https://jobs.eea.europa.eu/"),
    "scrape_era.py": ("European Railway Agency [ERA]", "https://www.era.europa.eu/agency-you/recruitment/vacancies"),
    "scrape_edps.py": ("European Data Protection Supervisor [EDPS]", "https://www.edps.europa.eu/about/office-edps/careers/our-vacancies_en"),
    "scrape_eiopa.py": ("European Insurance and Occupational Pensions Authority [EIOPA]", "https://eiopa.gestmax.eu/search"),
    "scrape_efsa.py": ("European Food Safety Authority [EFSA]", "https://careers.efsa.europa.eu/jobs/search"),
    "scrape_ecdc.py": ("European Centre for Disease Prevention and Control [ECDC]", "https://erecruitment.ecdc.europa.eu/?page=advertisement"),
    "scrape_fra.py": ("European Union Agency for Fundamental Rights [FRA]", "https://fra.gestmax.eu/search"),
    "scrape_eba.py": ("European Banking Authority [EBA]", "https://www.careers.eba.europa.eu/en"),
    "scrape_esma.py": ("European Securities and Markets Authority [ESMA]", "https://esmacareers.adequasys.com/?page=advertisement"),
    "scrape_iea.py": ("International Energy Agency [IEA]", "https://careers.smartrecruiters.com/OECD/iea"),
    "scrape_ceb.py": ("Council of Europe Development Bank [CEB]", "https://jobs.coebank.org/search"),
    "scrape_esa.py": ("European Space Agency [ESA]", "https://jobs.esa.int/search/"),
    "scrape_unesco.py": ("UNESCO", "https://careers.unesco.org/go/All-jobs-openings/784002/"),
    "scrape_wmo.py": ("World Meteorological Organization [WMO]", "https://erecruit.wmo.int/public/"),
    "scrape_easa.py": ("European Aviation Safety Agency [EASA]", "https://careers.easa.europa.eu/search/"),
    "scrape_eutelsat.py": ("EUTELSAT", "https://careers.eutelsat.com/search/"),
    "scrape_ebrd.py": ("European Bank for Reconstruction and Development [EBRD]", "https://jobs.ebrd.com/search/"),
    "scrape_eso.py": ("European Southern Observatory [ESO]", "https://recruitment.eso.org/"),
    "scrape_ema.py": ("European Medicines Agency [EMA]", "https://careers.ema.europa.eu/search/"),
    "scrape_ecmwf.py": ("European Centre for Medium-Range Weather Forecasts [ECMWF]", "https://jobs.ecmwf.int/Home/Job"),
    # Workday API scrapers
    "scrape_wto.py": ("World Trade Organization [WTO]", "https://wto.wd103.myworkdayjobs.com/External"),
    "scrape_embl.py": ("European Molecular Biology Laboratory [EMBL]", "https://embl.wd103.myworkdayjobs.com/EMBL"),
    "scrape_unhcr.py": ("UNHCR", "https://unhcr.wd3.myworkdayjobs.com/External"),
    "scrape_globalfund.py": ("The Global Fund", "https://theglobalfund.wd1.myworkdayjobs.com/External"),
    # EU institution scrapers
    "scrape_eu_careers.py": ("EU Careers", "https://eu-careers.europa.eu/en/non-permanent-contract-ec"),
    "scrape_vacancies.py": ("Anti-Money Laundering Authority [AMLA]", "https://www.amla.europa.eu/careers/vacancies_en"),
}


def load_scraper(filepath):
    spec = importlib.util.spec_from_file_location("scraper", filepath)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


def test_scraper(filename):
    filepath = SCRAPERS_DIR / filename
    if not filepath.exists():
        return None, f"File not found: {filepath}"

    name, url = SCRAPER_INFO.get(filename, (filename, ""))

    try:
        module = load_scraper(filepath)
        jobs = module.scrape()

        if jobs:
            return jobs, None
        else:
            return [], "No jobs found (may be empty or scraper issue)"
    except Exception as e:
        return None, str(e)


def main():
    print("Testing all scrapers...\n")

    for filename, (name, url) in SCRAPER_INFO.items():
        print(f"Testing {name}...")
        jobs, error = test_scraper(filename)

        if error:
            log_site(name, url, "failed", notes=error)
        elif len(jobs) == 0:
            log_site(name, url, "partial", script_file=f"scrapers/{filename}",
                     vacancies_found=0, notes="Scraper works but found 0 jobs")
        else:
            log_site(name, url, "success", script_file=f"scrapers/{filename}",
                     vacancies_found=len(jobs), notes=f"Found {len(jobs)} jobs")

    show_progress()


if __name__ == "__main__":
    main()
</file>

</files>
